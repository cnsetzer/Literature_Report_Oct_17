\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{soul,color}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{hypernat}
\DeclareRobustCommand{\hlcyan}[1]{{\sethlcolor{cyan}\hl{#1}}}
\usepackage[margin=21mm,bottom=23mm,top=21mm,paperwidth=210mm,paperheight=297mm,headsep=0.75cm]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\today}
\rhead{Christian N. Setzer}
\renewcommand\headrule{\vspace{3pt}\hrule height 1pt width\headwidth \vspace{-10pt}}
\title{Initial Literature Report}
\begin{document}
\hspace{-6mm}{\Large\bf{Literature Summary}}
\par

\vspace{7pt}
The Large Synoptic Survey Telescope (LSST) is a next generation telescope aimed at simultaneously probing many different regimes of astronomy and attempting to answer open questions from stellar science to cosmology. The main themes, as put forth by the LSST project are: Probe the nature of Dark Matter (DM) and Dark Energy (DE), explore the phenomenology and new timescales of the variable and transient universe, study the Milky Way and local stellar populations, and create a catalog of as many solar system objects as possible \cite{LSSTScienceCollaboration2017}. In this short review, the focus will be on the general cadence structure of the LSST ten-year survey, its interplay with the science cases, specifically transient science, and addressing the challenges of making use of the LSST for transient science through means of detection and classification. The bulk of the literature reviewed for this summary were the LSST Science Book vol. 2 and the LSST Observing Strategy Whitepaper \citep{LSSTScienceCollaboration2009,LSSTScienceCollaboration2017}. \par
Transient science with the LSST will be highly dependent on the ability to characterize the target populations of time-varying events. It will also be highly dependent on the cadence with which the survey area is sampled. The restrictions placed on these factors include our limited knowledge about the physics of these transient systems, the competition with other science cases for specific observing strategies and the physical limitations imposed by observing conditions and instrumentation. Before addressing the physical drivers for determination of the cadence from the perspective of transient science it is useful to review what the baseline cadence is and what some of the main science drivers are behind the project. \par
\section{General Cadence Considerations} %1.5 pages
The observational strategy, or cadence, that will be adopted for the LSST survey is still a work in progress. However, there are some main features which will be consistent throughout the optimization of cadence strategies. These features are primarily part of the Wide Fast Deep (WFD) survey strategy. What this actually entails are the generic features of the cadence which make this wide, fast, and deep. Naively these characteristics would seem to be something that any telescope would want the capability of having, but in this context it signifies that we are now in an era of Astronomy where all these goals can be achieved simultaneously. Covering a large sky area at a pace that would allow for the detection of changes in the sky on short timescales out to a distance that is beyond our local galactic environment make this a WFD survey \citep{LSSTScienceCollaboration2009}. \par
One of the main features of LSST that allows this to be accomplished is the telescope's large etendue, a measure of field of view (FOV) and light gathering power. LSST will have a larger etendue by order of magnitude than any previous survey. If etendue is a fiducial metric for science potential, as it is referred to in, \cite{LSSTScienceCollaboration2009}, then the LSST will be a prolific generator of science. For reference, the FOV of the LSST will be 9.6 \textit{deg}$^2$, or approximately 20 full moons. This large size was a requirement so that image quality would be atmospheric seeing limited, rather than optics limited. Additionally, the survey will operate in six filters, the u,g,r,i,z, and y filters similar to the Sloan Digital Sky Survey, with the addition of the sixth y filter \citep{LSSTScienceCollaboration2009}. A point that will be relevant later on, the filter system for LSST can only accomodate five filters consecutively. Without expending precious survey time on replacing one of the five filters for the sixth filter in a single night, the possible colors is limited to the filters chosen at the beginning of the night. Furthermore, switiching between any of the active filters requires a two-minute pause in observing which decreases the efficiency of the survey \citep{LSSTScienceCollaboration2017}. These factors will all be important when discussing identification and classification of transients later on. \par
The baseline cadence is not only the WFD survey, but it has been decided to set aside some of the available observing for mini-surveys such as the Deep Drilling Fields (DDF). While this is not the only mini survey, it is one with the highest priority of the additional surveys that will be completed. The DDF survey is also the only mini survey, out of those proposed in the current version of the Observing White Paper, which will explicitly help meet the science goals related to transient science. The DDF refer to a small selection of single telescope points spread throughout the sky that will be surveyed a much higher pace than the normal fields of the WFD survey. This is primarily to sample the area more frequently for early detection and better light curve sampling of transients, as will be discussed a bit more later on. Additionally, though not the primary intention, this enhanced sampling will provide a higher magnitude coadded five-sigma depth, probing the earlier universe \citep{LSSTScienceCollaboration2017}. \par
The current cadence, from here on labeled \textit{minion\_1016}, uses an approach that attempts to uniformly sample the entire visible sky area while accounting for many factors such as weather, the moon's path, maintenance time, allocation for the mini-surveys, among other factors. This however is not the only strategy proposed for implementing the survey. Another strategy is what is known as a rolling cadence. This is a very interesting idea because it aims to cluster visits to a smaller subset of fields, rather than the fully viewable night sky, and rotate this clustered region around the full survey area over the ten-year period. This decrease in full-survey uniformity has the benefit of increased sampling frequency of a desired region. This has the affect of providing better characterized lightcurves for the transient events that are detected at the expense of detecting a higher number of events. This allows for better science to be done with the data that is collected rather than simply relying on large statistics. A lot is unknown how this will affect the other science cases and much work is being done to explore the rolling cadence, but for transient science this would be a very suitable cadence strategy \citep{LSSTScienceCollaboration2017}. \par
One of the difficulties in evaluating a cadence is creating a consistent way to compare its impact on the diverse science cases that are of focus for the LSST. This is still a work in progress, but much thought has been put into constructing a set of metrics for each case which use variables intrinsic to the survey rather than the specific science case for easy comparison when cadence features change \citep{LSSTScienceCollaboration2017}. The idea is to compare a science case's metrics evaluated for the baseline cadence against other proposed cadences and then summarize these metrics with what is being called a Figure of Merit (FOM). The computational requirements for running the Operations Simulator (OPSIM) are high enough that an incremental exploration of the parameter space would be very difficult. In this way the comparison between cadence simulations explicitly rather than the explicit variation with a specific cadence feature is more useful and using a single FOM rather than a variety of metrics is more productive. While not the focus of discussion here, for the cases considered relevant metrics, or alternatively FOM, may be mentioned. \par
Lastly, when concerning the current cadence, there are some issues that are being addressed and will hopefully have a positive impact on all science cases concerned. These issues include a bias of observations toward the western night sky, rather than center on the meridian. This degrades visit depth due to poorer seeing at higher airmass. Two characteristics of the OPSIM which affect the predictions are the current models for sky brightness and limits on moon avoidance. The last major issue is the tendency to bias a large number of visits, five or more, to single field in a given night, thus increasing the inter-night gap between field visits and reducing the number of fields visited in a night \citep{LSSTScienceCollaboration2017}.  \par
\section{Transient Science Cadence Demands} % 0.5 page
While the explicit demands that transient science puts on the observational strategy have not been discussed, the discussion above about the baseline cadence and proposed rolling cadence address some of the issues, specifically high sampling frequency. This is a fairly straightforward requirement as transient science is inherently dealing with limited timescales. Exploring the physics of the two transient scenarios of interest, Supernovae (SNe) and Kilonovae (KNe), sometimes referred to in the literature as Macronovae (MNe), will determine requirements on the implemented observing strategy to obtain data suitable for science. This is seen through intrinsic lightcurve characteristics and variation timescales that constrain detection and classification. Particularly of interest is early-time observations and identification which place quite stringent requirements on an executed observing proposal. \par
Furthermore, other transient science cases may place even more stringent constraints on an executed cadence. These are namely Gamma Ray Burst (GRB) afterglows, though these are quite likley the same KNe events as evidenced by the recent BNS merger discovery , stellar flares, and Cataclysmic Variables (CV) \citep{Villar2017}. These all have relevant timescales less than a day which make capturing their variability a very difficult task for the WFD survey. These would potentially benefit greatly from the rolling cadence proposal. If we take the BNS merger discovery as typical of KNe events and the relevant timescales of SNe, then our cases on interest are on variational timescales from a couple hours to a few days \citep{LSSTScienceCollaboration2017}. A very general idea of the physics behind these timescales and the observations relevant for each case will be discussed next.\par
\section{Supernovae} %1 page
Supernovae make up one of the most important probes of cosmology. Particularly, through the ability to \"standardize\" type Ia SNe quite precise measurements can be made for the distances to these objects and thus a \"local\" universe measurement of $H_0$ can be made. However, the precision of these distance measurements relies heavily on the standardization process which, in turn, relies on the characterization of the lightcurve, except in the case of spectroscopically sampled SNe. The LSST will not have a spectroscopic instrument but it will be able to trigger other observatories for follow-up observations based on early detection and broad classification, i.e. SN as opposed to a local event like an asteroid or stellar flare. \par
Detection will be ultimately limited by the single-visit magnitude depth, or more optimistically the coadded single night depth if we accept that variation of a SN over one night is not too significant, and the brightest SNe in the FOV. Estimates with the current baseline observing strategy place the number of SNe discovered on the order of hundreds of thousands to several million covering a redshift range out to $z=1$ \citep{LSSTScienceCollaboration2017}. This clearly provides a vast amount of opportunity for science if the SNe can be properly classified. Classification in this sense is actually a two step process for SNe. I am referring to that first an object needs to be differentiated from other transients as a SN and further the type of SN must be resolved. \par
First, the issue of correct identification of a SN must be addressed. Without having a full lightcurve this can be done in a few ways. SNe are simply the brightest objects that we observe in the sky relative to their local environments, reaching absolute magnitudes greater than $-16$ for most of their life time, and peak magnitudes in the range of $-18$ to $-21$ \citep{LSSTScienceCollaboration2009}. Secondly, relatively few observations should be able to distinguish SNe from other transients. Their large distance essentially makes them a spatially static source and the sustained, or increasing brightness if detected early on, over the timescale of observations is a characteristic shared with only massive star eruptions, like Luminous Blue Variables, which are much more rare and have lower peak luminosity \citep{LSSTScienceCollaboration2017}. From a purely photometric point of view, then only a couple observations spread over a few days should be able to correclty identify any transient as a SN from sampling of a single lightcurve. \par
Identification of the type of SNe on the other hand requires more sampling of the object's lightcurve over a larger part of the lifetime of the transient. In all, the lightcurves for different types of SNe do not vary too greatly and thus to describe enough features of an object's lightcurve for reliable classification a well-sampled lightcurve is necessary. It should be immediately obvious that a rolling cadence would significantly help improve the size of such a sample of frequently sampled lightcurves \citep{LSSTScienceCollaboration2017}. How these lightcurves will be used to make the SN type identification will be discussed more later in the section regarding photometric classification. \par
With the observations and stresses on the cadence reviewed broadly, it is beneficial to breifly state a bit about the underlying physics of SNe. Generally, SNe can be characterized into two scenarios, thermonuclear fusion and core-collapse. These refer roughly to Type Ia SNe and everythign else, respectively. The detailed physics is incredibly complex but to get an understanding it can be said the thermonuclear fusion scenario is brought about by ignition of runaway carbon fusion in a white dwarf star, and the core-collapse scenario is the result of loss of radiation pressure supporting the core over very massive stars which causes a self-gravitating collapse and subsequent explosion \citep{Heger2002,Mazzali2007}. The actual mechanisms that drive these events are not very well known. SNe Ia generally show less variability between events which is why they have been so useful as standardizeable objects for cosmology. There is hope that with a more accurate description of the physics, particularly for a subset of core-collapse known as plateau SNe, that they may also be used for cosmology. \par
\section{Kilonovae} %1 page
Kilnovae are not very well understood from an observational perspective. There are a lot of predictions for their expected lightcurve behavior, which is relatively featureless \citep{Rosswog2016a}. However, only one such object has been observed that has been confirmed, via multi-messenger astronomy, to be a KN \citep{Kasliwal2017}. While one event is not enough to characterize a population, this event can be used to set limits on relevant time scales for better determination of lightcurves for futures observations. Similar to SNe, they require high frequency sampling. In fact the timescales relevant for early lightcurve evolution are on the order of hours, a much more rapid development than a typical SNe Ia \citep{Villar2017}. \par
As seen in the light curves produced for the recent Binary Neutron Star (BNS) merger, at optical detection on the order of ten hours past the gravitational wave triggering event was not able to capture the lightcurve peak \citep{Villar2017}. This places a very difficult requirement on any potential observing strategyt if any significant number is desired for science. Additionally, these events are much more rare than SNe, their rates are thought to be low-enough that estimates usually use a rate of only 300 per year per gigaparsec cubed volume \citep{Rosswog2016a}. However, one recent paper using a higher estimate of 1000 per year per gigaparsec cubed places an estimate on the total number that will be detected with the LSST to be approximately 75 \citep{Scolnic2017}.\par
In this scenario it becomes increasingly clear that a rolling cadence would be immensely improve detection efficiency. The sampling frequency of the WFD survey in the baseline cadence,field re-visit every three nights, would miss the significant features due to variations in the immediately post-explosion lightcurve. Using the recent BNS merger discovery as the benchmark for lightcurve behavior of future KN events, within the first ten days there is a decline in brightness within the range of 3 magnitude in y band to 8 magnitude in the u band. Ten days is significant here because in the observed lightcurves from this recent event the lightcurves in all bands exhibit much less variation after this date post-trigger \citep{Villar2017}. \par
As expected, with only one observed event, there is alot of the physics of KNe that need to be worked out. However, the basic physics have been quite well studied, beginning when stellar evolution indicated that the late stages of many stars is a neutron star and observations indicated that many stars evolve in binary systems. It is important to note that the physical scenario at play combines a lot of different terminology. The progenitors are BNS and potentially, though unobserved as of yet, neutron star blackhole binaries. Their catacylsmic merger immediately results in a gravitional wave signal and a gamma ray burst. The observed KN is then what is often referred to as the gamma ray burst afterglow. Modeling this merger results in a scenario comprised of jet and disc components around the merger remnant \citep{Rosswog2016a}. Though this work was done prior to the BNS merger discovery, it's predictions from this physical picture agree incredibly well with the observations \citep{Kasliwal2017}.\par

\section{Classification of Transients with LSST} % 0.5 page
It is easy to forget the distinction between detection of a transient and the classification of the event. Detection of an event is straightforward, using a standard template image of region, perform image differencing with the template and a new image and look at the remaining signal. If a signal exists beyond some threshold against the background noise this can confidently be labeled a detection. However, upon seeing a new event in the sky we cannot immediately say that it is a supernova or any other transient. This is because all photons are the same regardless of the source, in the sense that a photon isn't distinctly labeled as a SN photon etc. However, they do vary in spectral energy distribution and flux over time. These two facts alone allow us to distinguish one type of an event from another. \par
The challenge is, when knowledge of the intrinsic variations between sources is limited, or the data describing the event is limited, how are different events able to be distinguished from one another. This is where machine learning becomes incredibly useful. As discussed in \cite{Lochner2016}, there are many approaches to using machine learning on the type of dataset that the LSST will generate. As was mentioned already, determining the type of detection, i.e. classification of the event, is more difficult.
\subsection{Photometric Classification} %1.5 page

\section{Areas for Future Work} % 1 page
Better understanding of what a representative sample for Kilonovae.
Understand the desired sampling for photometric classification using SNmachine. Which parts of the lightcurve need to be sampled. Does this affect SN or KN classification?
KNe lightcurves are quite featureless. Would color curves help? Colors do evolve faster than individual lightcurves, but show what seem to be more \"features\".
Considering the detection of a population of KNe, what cosmology can be done with them?
What would a ToO program for the LSST actually look like?

\newpage
\bibliography{PhD-Main_Project-Lit_Rep}
\bibliographystyle{aasjournal}

\end{document}
