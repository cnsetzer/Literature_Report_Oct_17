\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{soul,color}
\usepackage{hyperref}
\usepackage{natbib}
\DeclareRobustCommand{\hlcyan}[1]{{\sethlcolor{cyan}\hl{#1}}}
\usepackage[margin=21mm,bottom=23mm,top=21mm,paperwidth=210mm,paperheight=297mm,headsep=0.75cm]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\today}
\rhead{Christian N. Setzer}
\renewcommand\headrule{\vspace{3pt}\hrule height 1pt width\headwidth \vspace{-10pt}}
\title{Initial Literature Report}
\begin{document}
\hspace{-6mm}{\Large\bf{Literature Summary}}
\par
\vspace{7pt}
The Large Synoptic Survey Telescope (LSST) is a next generation telescope aimed at simultaneously probing many different regimes of astronomy and attempting to answer open questions from stellar science to cosmology. The main themes, as put forth by the LSST project are: Probe the nature of Dark Matter (DM) and Dark Energy (DE), explore the phenomenology and new timescales of the variable and transient universe, study the Milky Way and local stellar populations, and create a catalog of as many solar system objects as possible \cite{LSSTScienceCollaboration2017}. In this short review, the focus will be on the general cadence structure of the LSST ten-year survey, its interplay with the science cases, specifically transient science, and addressing the challenges of making use of the LSST for transient science through means of detection and classification. The bulk of the literature reviewed for this summary were the LSST Science Book vol. 2 and the LSST Observing Strategy Whitepaper \citep{LSSTScienceCollaboration2009,LSSTScienceCollaboration2017}. \par
Transient science with the LSST will be highly dependent on the ability to characterize the target populations of time-varying events. It will also be highly dependent on the cadence with which the survey area is sampled. The restrictions placed on these factors include our limited knowledge about the physics of these transient systems, the competition with other science cases for specific observing strategies and the physical limitations imposed by observing conditions and instrumentation. Before addressing the physical drivers for determination of the cadence from the perspective of transient science it is useful to review what the baseline cadence is and what some of the main science drivers are behind the project. \par

\section{General Cadence Considerations} %1.5 pages
The observational strategy, or cadence, that will be adopted for the LSST survey is still a work in progress. However, there are some main features which will be consistent throughout the optimization of cadence strategies. These features are primarily part of the Wide Fast Deep (WFD) survey strategy. What this actually entails are the generic features of the cadence which make this wide, fast, and deep. Naively these characteristics would seem to be something that any telescope would want the capability of having, but in this context it signifies that we are now in an era of Astronomy where all these goals can be achieved simultaneously. Covering a large sky area at a pace that would allow for the detection of changes in the sky on short timescales out to a distance that is beyond our local galactic environment make this a WFD survey \citep{LSSTScienceCollaboration2009}. \par
One of the main features of LSST that allows this to be accomplished is the telescope's large etendue, a measure of field of view (FOV) and light gathering power. LSST will have a larger etendue by order of magnitude than any previous survey. If etendue is a fiducial metric for science potential, as it is referred to in, \cite{LSSTScienceCollaboration2009}, then the LSST will be a prolific generator of science. For reference, the FOV of the LSST will be 9.6 \textit{deg}$^2$, or approximately 20 full moons. This large size was a requirement so that image quality would be atmospheric seeing limited, rather than optics limited. Additionally, the survey will operate in six filters, the u,g,r,i,z, and y filters similar to the Sloan Digital Sky Survey, with the addition of the sixth y filter \citep{LSSTScienceCollaboration2009}.   \par
The baseline cadence is not only the WFD survey, but it has been decided to set aside some of the available observing for mini-surveys such as the Deep Drilling Fields (DDF). While this is not the only mini survey, it is one with the highest priority of the additional surveys that will be completed. The DDF survey is also the only mini survey, out of those proposed in the current version of the Observing White Paper, which will explicitly help meet the science goals related to transient science. The DDF refer to a small selection of single telescope points spread throughout the sky that will be surveyed a much higher pace than the normal fields of the WFD survey. This is primarily to sample the area more frequently for early detection and better light curve sampling of transients, as will be discussed a bit more later on. Additionally, though not the primary intention, this enhanced sampling will provide a higher magnitude coadded five-sigma depth, probing the earlier universe \citep{LSSTScienceCollaboration2017}. \par
The current cadence, from here on labeled \textit{minion\_1016}, uses an approach that attempts to uniformly sample the entire visible sky area while accounting for many factors such as weather, the moon's path, maintenance time, allocation for the mini-surveys, among other factors. This however is not the only strategy proposed for implementing the survey. Another strategy is what is known as a rolling cadence. This is a very interesting idea because it aims to cluster visits to a smaller subset of fields, rather than the fully viewable night sky, and rotate this clustered region around the full survey area over the ten-year period. This decrease in full-survey uniformity has the benefit of increased sampling frequency of a desired region. This has the affect of providing better characterized lightcurves for the transient events that are detected at the expense of detecting a higher number of events. This allows for better science to be done with the data that is collected rather than simply relying on large statistics. A lot is unknown how this will affect the other science cases and much work is being done to explore the rolling cadence, but for transient science this would be a very suitable cadence strategy \citep{LSSTScienceCollaboration2017}. \par
One of the difficulties in evaluating a cadence is creating a consistent way to compare its impact on the diverse science cases that are of focus for the LSST. This is still a work in progress, but much thought has been put into constructing a set of metrics for each case which use variables intrinsic to the survey rather than the specific science case for easy comparison when cadence features change \citep{LSSTScienceCollaboration2017}. The idea is to compare a science cases metric evaluated for the baseline cadence against other proposed cadences. The computational requirements for running the Operations Simulator (OPSIM) are high enough that an incremental exploration of the parameter space would be very difficult. In this way the comparison between cadence simulations explicitly rather than the explicit variation with a specific cadence feature is more useful. While not the focus of discussion here, for the cases considered, current metrics will be mentioned. \par

\section{Cadence Demands} % 0.5 page
While the explicit demands that transient science puts on the observational strategy have not been discussed, the discussion above about the baseline cadence and proposed rolling cadence address some of the issues, specifically high sampling frequency. This is a fairly straightforward requirement as transient science is inherently dealing with limited timescales. Exploring the physics of the two transient scenarios of interest, Supernovae (SNe) and Kilonovae (KNe), sometimes referred to in the literature as Macronovae (MNe), will determine requirements on the implemented observing strategy to obtain data suitable for science. This is seen through intrinsic lightcurve characteristics and variation timescales that constrain detection and classification. Particularly of interest is early-time observations and identification which place quite stringent requirements on an executed observing proposal.  \par

\subsection{Supernovae} %1 page
Supernovae make up one of the most important probes of cosmology. Particularly through the ability to "standardize" type Ia SNe, relatively precise measurements can be made for the distances to these objects. However, the precision of these distance measurements relies heavily on the standardization process which, in turn, relies on the characterization of the lightcurve, except in the case of spectroscopically sampled SNe. The LSST will not have a spectroscopic instrument but it will be able to trigger other observatories for follow-up observations based on early detection and broad classification, i.e. SN as opposed to a local event like an asteroid or stellar flare. \par
An important 

\subsection{Kilonovae} %1 page
Kilnovae are not very well understood from an observational perspective. There are a lot of predictions for their expected lightcurve behavior, which is relatively featureless \citep{Rosswog2016a}. However, only one such object has been observed that has been confirmed, via multi-messenger astronomy, to be a Kilonova. While one event is not enough to characterize a population, this event can be used to set limits on relevant time scales for better determination of lightcurves for futures observations \citep{Villar2017}. Similar to SNe, they require high frequency sampling. In fact the timescales relevant for early lightcurve evolution are on the order of hours, a much more rapid development than a typical SNe Ia. \par
As seen in the light curves produced for the recent Binary Neutron Star (BNS) merger, at optical detection on the order of ten hours past the gravitational wave triggering event was not able to capture the lightcurve peak \cite{Villar2017}.

\section{Classification of Transients with LSST} % 0.5 page
It is easy to forget the distinction between detection of a transient and the classification of the event. Detection of an event is straightforward, using a standard template image of region, perform image differencing with the template and a new image and look at the remaining signal. If a signal exists beyond some threshold against the background noise this can confidently be labeled a detection. However, upon seeing a new event in the sky we cannot immediately say that it is a supernova or any other transient. This is because all photons are the same regardless of the source, in the sense that a photon isn't distinctly labeled as a SN photon etc. However, they do vary in spectral energy distribution and flux over time. These two facts alone allow us to distinguish one type of an event from another. \par
The challenge is, when knowledge of the intrinsic variations between sources is limited, or the data describing the event is limited, how are different events able to be distinguished from one another. This is where machine learning becomes incredibly useful. As discussed in \cite{Lochner2016}, there are many approaches to using machine learning on the type of dataset that the LSST will generate. As was mentioned already, determining the type of detection, i.e. classification of the event, is more difficult. 
\subsection{Photometric Classification} %1.5 page

\section{Areas for Future Work} % 1 page
Better understanding of what a representative sample for Kilonovae.
Understand the desired sampling for photometric classification using SNmachine. Which parts of the light curve are sampled. Does this affect SN or KN classification?
Lightcurves are quite featureless. Would color curves help? Colors do evolve faster than individual lightcurves, but show what seem to be more "features".
Considering the detection of a population of KNe, what cosmology can be done with them? 

\bibliography{PhD_Main_Project}
\bibliographystyle{aasjournal}

\end{document}

